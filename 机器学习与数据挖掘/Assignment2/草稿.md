### CIFAR10数据集：

* 标签：0-9, <code>  print(np.max(Ytrain))</code>
* 一个批次标签热编码：<code>torch.eye(10)</code>生成一个10*10的矩阵 [labels]索引处对应行，最后出来应该是10000 \* 10,标签0，对应的热向量[1,0,0...0]，调用交叉熵时不用传入热向
* 总训练标签：Y_train:50000，
* 总训练样本特征：X_train:50000\*3\*32\*32 =50000*3072
* 一个批次内数据的存储：10000（样本数）x3072 （1024*3个通道值）numpy阵列的uint8。数组的每一行存储一个32x32的彩色图像。前1024项包含红色通道值，中间1024项包含绿色通道值，最后1024项包含蓝色通道值。图像按以行为主（row-major）的顺序存储，因此数组的前32个条目是图像第一行的红色通道值
* 一个批次内数据的重塑：
  <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20241115100154988.png" alt="image-20241115100154988" style="zoom:50%;" />

### softmax

* 对样本X的预测概率：
  $$
  f_i(x)=softmax_i(xW)={e^{xw_i} \over \sum_{k=1}^K e^{xw_k} }\\
  w_k是行向量
  $$
  

​		<img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20241115091531912.png" alt="image-20241115091531912" style="zoom:50%;" />

* 需要对真实标签读热向量转化
  <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20241115092043684.png" alt="image-20241115092043684" style="zoom:50%;" />

* Loss函数：就是交叉熵
  $$
  L(w_1,w_2,...w_k)=-{1\over N}\sum_{l=1}^N \sum_{k=1}^K y_k^{(l)} log[softmax_k(x^{l}W)]
  $$

* 梯度下降：

  <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20241115093108053.png" alt="image-20241115093108053" style="zoom:50%;" />

  

* pytorch实现
  ![image-20241115094220792](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20241115094220792.png)

* 要对特征值归一化：

  ```
  让我们来分析一下这个简单的网络结构：
  
  全连接层：这一层将输入数据线性变换为一系列输出值。这些输出值随后被传递给Softmax层。
  Softmax层：这一层将全连接层的输出转换为概率分布。Softmax函数通过计算每个输出值的指数与所有输出值指数之和的比例来实现这一点。
  现在，考虑输入数据范围大的情况：
  
  如果输入数据的范围非常大（比如从-1000到1000），那么全连接层的输出（即Softmax层的输入）也可能非常大（或非常小，取决于数据的正负）。
  Softmax函数本身并不直接受输入数据范围大的影响，因为它总是将输入转换为概率分布。然而，如果Softmax层的输入非常大或非常小，那么这些输入值在Softmax函数中的指数运算后可能会变得极端（即非常接近0或1）。
  当Softmax层的某个输入值变得非常大时，它在Softmax函数中的输出（即对应类别的概率）将非常接近1，而其他类别的概率将非常接近0。这意味着，对于该输入数据点，神经网络已经“确信”它属于某个类别，并且很难通过梯度下降来调整这个决策（因为梯度会非常小）。
  在反向传播过程中，如果Softmax层的输出非常接近1或0，那么其梯度（即损失函数对Softmax层输入的导数）将非常接近0。这将导致全连接层的权重更新变得非常缓慢或停滞不前，因为梯度在反向传播到全连接层时已经被大大衰减。
  ```


```
def CNN_train_test(Train_loader,Test_loader):
    CNN_model=CNNClassifier()
    loss_func=nn.CrossEntropyLoss()
    optimizer=optim.Adam(CNN_model.parameters(),lr=0.001)#0.01不收敛
    CNN_model = CNN_model.to(device)#gpu训练
    EPOCHES=50#迭代次数为20
    for epoch in range(EPOCHES):
     for step,(b_x,b_y) in enumerate(Train_loader):#step为批次索引，b_x为每一批次的输入，b_y为每一批次的标签
        # 将数据和标签移动到GPU
        optimizer.zero_grad() #清除梯度
        b_x = b_x.to(device) 
        b_y = b_y.to(device)  
        output=CNN_model(b_x)  #输入每一批次到模型，自动调用forward
        loss=loss_func(output,b_y) #计算损失函数
        loss.backward() #损失函数的反向传播
        optimizer.step() #参数优化
     for step2,(test_x,test_y) in enumerate(Test_loader): #一个批次弄完
                test_x = test_x.to(device)  
                test_y = test_y.to(device)               
                test_output = CNN_model(test_x)
                pred_y = torch.max(test_output, 1)[1].data.detach().cpu().numpy()  #GPU数据移到cpu再移到numpy  #返回概率最大的索引
                accuracy = float((pred_y == test_y.data.detach().cpu().numpy()).astype(int).sum()) / float(test_y.size(0))
                print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.detach().cpu().numpy(), '| test accuracy: %.2f' % accuracy)
                


def MLP_train_test(Train_loader,Test_loader,option,lr=0.0005,momentum=0.9,betas=(0.9,0.999),EPOCHES=100):
    #1.定义模型
    MLP_model=MLPClassifier(3072,10)
    loss_func=nn.CrossEntropyLoss()
    optimizer=optim.Adam(MLP_model.parameters(),lr=0.0001)#0.01不收敛
    MLP_model = MLP_model.to(device)#gpu训练
    for epoch in range(EPOCHES):
     for step,(b_x,b_y) in enumerate(Train_loader):#step为批次索引，b_x为每一批次的输入，b_y为每一批次的标签
        # 将数据和标签移动到GPU
        optimizer.zero_grad() #清除梯度
        b_x = b_x.to(device)  #torch.Size([64, 3072]) 
        b_y = b_y.to(device)  #torch.Size([64])
        output=MLP_model(b_x)  #输入每一批次到模型，自动调用forward
        loss=loss_func(output,b_y) #计算损失函数
        loss.backward() #损失函数的反向传播
        optimizer.step() #参数优化
     for step2,(test_x,test_y) in enumerate(Test_loader): #一个批次弄完
                test_x = test_x.to(device)  
                test_y = test_y.to(device)               
                test_output = MLP_model(test_x)
                pred_y = torch.max(test_output, 1)[1].data.detach().cpu().numpy()  #GPU数据移到cpu再移到numpy  #返回概率最大的索引
                accuracy = float((pred_y == test_y.data.detach().cpu().numpy()).astype(int).sum()) / float(test_y.size(0))
                print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.detach().cpu().numpy(), '| test accuracy: %.2f' % accuracy)

```

