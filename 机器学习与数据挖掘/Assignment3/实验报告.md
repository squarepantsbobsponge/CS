![image-20240506113933705](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240506113933705.png)

 

 

​												本科生实验报告

​										学生姓名：      丁晓琪

​										学生学号：       22336057

​										专业名称：	计科

[TOC]

## 一：PCA降低图像特征维度

由于样本的图像为784维，则需要对图像降维处理
#### (1). 算法流程

1. 对训练样本进行数据中心化，$X_{\text{centered}} = X - \bar{X}$
2. 求训练数据中心化后的协方差矩阵：$ C = \frac{1}{m} X_{\text{centered}}^T X_{\text{centered}}$
3. 对该协方差矩阵计算特征值和特征向量
   * 选择主成分：根据特征值的大小，选择前 *n*components 个最大的特征值对应的特征向量作为主成分。这些特征向量构成了一个转换矩阵 *W*。

4. 数据转化：将原始训练数据和原始测试数据投影到主成分空间上，得到降维后的数据$   X_{\text{reduced}} = X_{\text{centered}} W$​


#### (2). 具体实现

```python
class PCA():
    def __init__(self):
        self.mean=None
        self.components=None
    
    def fit(self,X,n_components):
        # 功能：根据X计算n_components个主成分向量
        m,n=X.shape
        # 1 求协方差矩阵
        self.mean=np.mean(X,axis=0)
        X_centered=X-self.mean #会广播的
        covariance_matrix=np.matmul(X_centered.T,X_centered)/m
        # 2.计算特征值和特征向量（列向量）
        eigenvalues,eigenvectors=np.linalg.eig(covariance_matrix)
        # 3.取出最大的前n个特征向量
            #将特征向量最大到小的索引找出来
        index=np.argsort(eigenvalues)[::-1]
            #排序
        eigenvectors=eigenvectors[:,index]
        #   选择前n个特征向量
        self.components=eigenvectors[:,:n_components]

    def transform(self,X):
        # 功能：将向量转到主成分空间
        return np.dot(X-self.mean,self.components)
```

zh

## 二：Kmeans

#### (1).算法流程(参数初始化方法)：

1. 初始化阶段：选择k个初始聚类中心（如果要划分为k个聚类）

   聚类中心的初始化：

   * 法一：随机选择训练样本内的k个顶点为k个聚类的初始化中心

     ```python
         def init_center_random(self):
             # 功能：随机从训练实例中挑选10个样本初始化聚类中心
             init_num = np.random.choice(self.train_images.shape[0], self.k, replace=False)
             self.centers= self.train_images[init_num]
     ```

   * 法二：

     * 思想：依据距离初始化聚类中心，使得聚类中心尽可能分散。

     * 实现：先随机从样本中挑选出一个点为第一个聚类中心，然后在样本中选出离当前已存在聚类中心最远的样本为新的聚类中心，直到存在k个聚类中心为止

       ```python
           def init_center_distance(self):
               # 功能：根据距离初始化聚类中心
               # 1. 选择第一个中心点为随机样本
               self.centers = np.zeros((self.k, self.train_images.shape[1]))
               self.centers[0] = self.train_images[np.random.choice(self.train_images.shape[0])]
               # 2. 计算所有点到第一个中心点的距离
               distances = np.linalg.norm(self.train_images - self.centers[0], axis=1)
               # 3. 选择剩余的中心点
               for i in range(1, self.k):
                   # 选择与已有中心点距离最远的点作为新的中心点
                   farthest_index = np.argmax(distances)
                   self.centers[i] = self.train_images[farthest_index]
                   # 更新距离，考虑新中心点
                   new_distances = np.linalg.norm(self.train_images - self.centers[i], axis=1)
                   # 更新到新中心点的距离
                   distances = np.minimum(distances, new_distances)
       ```

       

2. 迭代阶段：
       在`Kmeans.train()`中实现:

   * 计算数据集中每个点到K个聚类中心的距离，使用欧氏距离作为度量标准

     ```python
         def compute_closest_index(self, sample):
             # 功能：计算并返回样本sample距离（欧几里得距离）最近的聚类中心的索引
             # 输入：
             #   sample：要计算的样本
             distances = np.sum((self.centers - sample) ** 2, axis=1)
             closest_center_index = np.argmin(distances)
             return closest_center_index
     ```

     将每个点分配给距离其最近的聚类中心，形成K个聚类

     ```python
             # 1.对样本判断所属簇
              # 1.1 创建数组来存储每个样本的簇分配 
                 cluster_assignments = np.zeros(self.train_images.shape[0], dtype=int)
                 for i in range(0,self.train_images.shape[0]):
                     # 计算样本到每个聚类中心的距离
                     # 找到最近的聚类中心的索引
                     closest_index=self.compute_closest_index(self.train_images[i])
                     cluster_assignments[i]=closest_index
     ```

   * 对于每个聚类，重新计算其聚类中心，即计算该聚类中所有点的均值作为新的聚类中心(聚类内所有点在每个维度上的均值是该聚类新的中心的取值)

     ```python
                 # 2: 更新聚类中心
                 new_centers = np.zeros((self.k, self.train_images.shape[1]))
                 for i in range(0,self.k):
                     # 找到第i簇的所有样本
                     cluster_i_sample=self.train_images[cluster_assignments==i]
                     # 簇不为空时，更新聚类中心
                     if cluster_i_sample.size>0:
                         new_centers[i]=cluster_i_sample.mean(axis=0)
                 self.centers=new_centers
                 self.cluster_assignment=cluster_assignments
     ```

3. 测试阶段：

   训练出k个聚类中心后，聚类中心在`self.center`中的索引并不代表聚类的标签。由于训练的样本是有标签的数据，则这里取最后一次训练过程中被分到聚类内的训练样本的标签众数为该聚类的标签。（训练样本的被分配到的聚类中心的索引存储在`self.cluster_assignment`中）

   ```python
       def test(self,test_labels,test_images):
           # 功能：用test_images样本和它对应的标签test_labels测试模型，计算模型正确率
           # 输入：
           #   test_images: 测试样本
           #   test_labels: 测试样本的标签
           correct_sum=0
           for i in range(0,test_images.shape[0]):
                   # 1.计算样本到每个聚类中心的距离
                   #   找到最近的聚类中心的索引
                   closest_index=self.compute_closest_index(test_images[i])
                   # 2. 找该聚类的标签（聚类内样本标签的众数）
                   cluster_labels = self.train_labels[self.cluster_assignment == closest_index]
                   mode_result = mode(cluster_labels)
                   predict_label = mode_result.mode[0]  
                   # 3.比较聚类标签和样本真实标签
                   if predict_label==test_labels[i]:
                       correct_sum+=1
           accuracy = correct_sum / test_images.shape[0]
           self.test_accuracies.append(accuracy)
   ```

#### (2). 实验结果

* 实验结果获取：

  * 分别采用两种初始化方法初始化参数，并且训练聚类，每次训练都迭代50次

  * 在模型训练过程中：每隔5次迭代就会用`test_image`训练集测试聚类的正确性

  * 记录从聚类参数初始化到训练聚类结束的总训练时间（包括训练过程中测试聚类正确性的时间）

    ```python
        kmeans_random=Kmeans(10,50,train_labels=train_labels,train_images=train_images)
        start_time = time.time()
        kmeans_random.init_center_distance()
        kmeans_random.train(test_images=test_images,test_labels=test_labels)
        end_time = time.time()
        training_duration = end_time - start_time
        print(f"训练过程耗时 {training_duration:.2f} 秒")
        print(f"final test accruacy: {kmeans_random.test(test_labels=test_labels,test_images=test_images)}")
        kmeans_random.plot_accuracies("init_random_distance_K_means_Accuracies")
    
    
        kmeans_center=Kmeans(10,50,train_labels=train_labels,train_images=train_images)
        start_time = time.time()
        kmeans_center.init_center_distance()
        kmeans_center.train(test_images=test_images,test_labels=test_labels)
        end_time = time.time()
        training_duration = end_time - start_time
        print(f"训练过程耗时 {training_duration:.2f} 秒")
        kmeans_center.test(test_labels=test_labels,test_images=test_images)
        print(f"final test accruacy: {kmeans_random.test(test_labels=test_labels,test_images=test_images)}")
        kmeans_center.plot_accuracies("init_center_distance_K_means_Accuracies")
    ```

* 实验结果：

  |                              | 随机初始化聚类中心                                           | 根据距离初始化聚类中心                                       |
  | ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | 训练总时长                   | <img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241209221059308.png" alt="image-20241209221059308" style="zoom:60%;" /> | <img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241209221117873.png" alt="image-20241209221117873" style="zoom:67%;" /> |
  | 训练过程中的正确率           | <img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241209220303128.png" alt="image-20241209220303128" style="zoom:50%;" /><br /><img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241209220904109.png" alt="image-20241209220904109" style="zoom:50%;" /> | <img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241209220428987.png" alt="image-20241209220428987" style="zoom:50%;" /><br /><img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241209221011901.png" alt="image-20241209221011901" style="zoom:50%;" /> |
  | 训练完的聚类对测试集的正确率 | 60.01%<br /><br /><img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241209220942379.png" alt="image-20241209220942379" style="zoom:50%;" /> | 60.01%<br /><img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241209221040117.png" alt="image-20241209221040117" style="zoom:50%;" /> |
  |                              |                                                              |                                                              |

* 实验结果分析：

  * 训练总时长：随机初始化聚类中心比根据距离初始化聚类中心短。

    原因：根据距离初始化聚类中心在初始化阶段需要迭代对每个样本进行和聚类中心的距离计算，则耗时较长

  * 正确率：

    * 训练开始时的正确率：根据距离初始化比随机初始化的正确率高

      原因：根据距离初始化的初始聚类中心尽可能分散，可能更接近真实样本的分类

    * 训练过程的正确率：根据距离初始化在第10次迭代收敛达到最高值60.62%比随机初始化的第45次迭代60.11%更快且正确率更高。根据距离初始化在15次迭代左右开始会由于迭代次数过多出现过拟合导致正确率下降
      原因：根据距离初始化的初始聚类中心尽可能分散，可能更接近真实样本的分类
    * 最终正确率：两个初始化方法的正确率相近甚至相同

## 三：EM算法训练GMM模型

#### (1).算法流程

GMM模型：p(x|θ)=∑k=1Kπk𝒩(x|μk,Σk)

* GMM包含K个参数模型的概率密度函数，这些函数的加权组合构成了GMM的概率密度函数。
* 在GMM中，每个高斯分布都代表了一个聚类，而整个模型则能够描述数据的多峰分布特性。
* GMM引入了隐变量z，用于表示数据属于哪一个聚类。

* 计算样本在高斯分布中的概率

  ```python
          mvn = multivariate_normal(self.means[index], self.covariance_matrices[index])
          return mvn.pdf(sample)
  ```

1. 初始化：

   * 高斯分布的数量：指定为10

   * 每个高斯分布的均值：同上面的kmeans获取10个聚类中心的两种初始化方法：随机初始化和根据距离初始化

     ```python
             # 2. 初始化每个高斯分布的均值
             #   随机抽取k个样本初始化均值
             if(means_init_method==0):
                 init_num = np.random.choice(self.train_images.shape[0], self.k, replace=False)
                 self.means= self.train_images[init_num]
             # 根据距离来选择哪个样本作为高斯分布的初始化均值
             elif(means_init_method==1):
                 # 选择第一个平均值为随机样本
                 self.means = np.zeros((self.k, self.train_images.shape[1]))
                 self.means[0] = self.train_images[np.random.choice(self.train_images.shape[0])]
                 # 计算所有点到第一个中心点的距离
                 distances = np.linalg.norm(self.train_images - self.means[0], axis=1)
                 # 选择剩余的中心点
                 for i in range(1, self.k):
                     # 选择与已有中心点距离最远的点作为新的中心点
                     farthest_index = np.argmax(distances)
                     self.means[i] = self.train_images[farthest_index]
                     # 更新距离，考虑新中心点
                     new_distances = np.linalg.norm(self.train_images - self.means[i], axis=1)
                     # 更新到新中心点的距离
                     distances = np.minimum(distances, new_distances)
     ```

   * 每个高斯分布的权重：随机生成整数并归一化的方法

     ```python
             random_weights = np.random.randint(0, 101, size=10)
             weights_sum = np.sum(random_weights.astype(float))
             self.weights = random_weights.astype(float) / weights_sum
     ```

   * 每个高斯分布的协方差矩阵：
     * 法一：使用样本的协方差矩阵的对角矩阵进行初始化
     
       ```python
               # 3. 初始化每个高斯分布是协方差矩阵
               # 要初始化为样本的协方差矩阵的对角矩阵(对角元素不同)
               if(cov_init_method==0):
                   cov = np.cov(self.train_images, rowvar=False) + 1e-6 * np.eye(self.train_images.shape[1])
                   cov = np.diag(np.diag(cov))
                   self.covariance_matrices= cov[np.newaxis, :].repeat(self.k, axis=0)
       ```
     
     * 法二：始化为一个所有对角元素相等的对角矩阵。
     
       ```python
               # 3. 初始化每个高斯分布是协方差矩阵
                   #初始化对角矩阵元素都相等，球形初始化
                   avg_var = np.mean(np.var(self.train_images, axis=0))
                   cov = avg_var * np.ones((self.train_images.shape[1], self.train_images.shape[1]))
                   # 由于cov需要是对角矩阵，这里仍使用对角矩阵形式
                   cov = np.diag(np.full(self.train_images.shape[1], avg_var))
                   self.covariance_matrices = cov[np.newaxis, :].repeat(self.k, axis=0)
       ```

2. EM算法迭代：

   每次迭代交替做E-M步

   * E步：

     * 通过计算每个样本在每个高斯分布下的后验概率（已知样本x，x在第i个高斯分布中出现的概率）来计算落在第k个高斯分布的概率

     $$
     \gamma_{jk}=E_{p(z|x;\theta^{(t)}[z_k])} = \frac{\alpha_k N(x|\mu_k,\Sigma_k)}{\sum_{l=1}^{K}\alpha_l N(x|\mu_l,\Sigma_l)}=p(z=1_k|x;\theta^{(t)})
     $$

     ​	其中，$N(x_j|\mu_k,\Sigma_k)$​是样本`x_j`在第`k`个高斯分布下的概率密度函数。

     ```python
                 # 1.计算gamma
                 gamma_matrix=np.zeros((self.train_images.shape[0],self.k))
                 for j in range(0,self.k):
                     # 一次性计算所有样本的概率密度
                     gamma_matrix[:, j] = self.Gaussion_probability(j,self.train_images) * self.weights[j]
                 #print(gamma_matrix)
                 gamma_matrix/=gamma_matrix.sum(axis=1,keepdims=True) #按行求和保持维度
     ```

   * M步：N为样本特征值维度

     * 根据E步得到的γ矩阵，更新高斯分布的参数（权重、均值、协方差矩阵）。

     * 计算`N_k`: $$ N_k=\sum_{j=1}^{N}\gamma_{jk}$$​

       ```python
       N=np.sum(gamma_matrix,axis=0) #按行求和
       ```

       

     * 更新权重`α_k`：$$\alpha_k = \frac{N_k}{N}$$​

       ```python
       self.weights[i]=N[i]/self.train_images.shape[0]
       ```

       

     * 更新每个高斯分布的均值`u_k`：$$\mu_k = \frac{\sum_{j=1}^{N}\gamma_{jk}x_j}{N_k}$$​

       ```python
       self.means[i]=(1/N[i])*np.dot(gamma_matrix[:,i].reshape(1,-1),self.train_images) 
       ```

       

     * 更新协方差矩阵`Σ_k`：

       $$ \Sigma_k = \frac{\sum_{j=1}^{N}\gamma_{jk}(x_j - \mu_k)^T(x_j - \mu_k)}{N_k} $$​

       ```python
       covariance_matrix = (1 / (N[i] + 1e-6)) * np.dot((gamma_matrix[:, i].reshape(-1, 1) * diff).T, diff)                
       self.covariance_matrices[i]=covariance_matrix
       ```

3. 测试阶段：同k_means中的测试方法，用最后一次训练的分到各个高斯分布的训练样本的标签众数为该高斯分布的预测标签。计算测试样本在各个高斯分布下的概率，取最大概率的高斯分布的所属标签为测试样本的预测标签

   ```python
       def test(self,test_images,test_labels):
           # 功能：已知x求属于哪个聚类z
           correct_sum=0
           gamma_vector=np.zeros((test_images.shape[0],self.k))
           for j in range(0,self.k):
                   # 计算每个样本在第j个高斯分布下的概率    
               gamma_vector[:, j] = self.Gaussion_probability(j, test_images) * self.weights[j]
   
           gamma_vector /= np.sum(gamma_vector, axis=1, keepdims=True)
           predict_test_indexs=np.argmax(gamma_vector,axis=1)
           for i in range(0,test_images.shape[0]):
               cluster_labels = self.train_labels[self.predict_indexs == predict_test_indexs[i]]
               mode_result = mode(cluster_labels)
               predict_label = mode_result.mode[0] 
               #print(i)
               if predict_label == test_labels[i]:
                   correct_sum += 1
           
           accuracy = correct_sum / test_images.shape[0]
           self.test_accuracies.append(accuracy)
           return accuracy
   ```

#### (2). 实验结果

对上述提到的不同初始化方法分别训练迭代100次。

在训练过程中每隔5次迭代用测试集测试模型的正确率。
<img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241210165220618.png" alt="image-20241210165220618" style="zoom:80%;" />

|                                                              | 随机初始化高斯分布的均值                                     | 根据距离初始化高斯分布的均值                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 以训练样本的协方差矩阵的对角矩阵为高斯分布的协方差矩阵       | <img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241210165023560.png" alt="image-20241210165023560" style="zoom:50%;" /><br />收敛速度：最慢 | <img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241210165054131.png" alt="image-20241210165054131" style="zoom:50%;" /><br />训练时间最长：计算训练样本呢的协方差矩阵和计算每个样本到聚类的距离花费的时间最长<br />正确率：最高 |
| 以训练样本某个维度的平均值为对角矩阵的元素的对角矩阵为高斯分布的协方差矩阵 | <img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241210165036618.png" alt="image-20241210165036618" style="zoom:50%;" /><br />最快收敛 | <img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241210165110325.png" alt="image-20241210165110325" style="zoom:50%;" /><br /><img src="C:/Users/丁晓琪/AppData/Roaming/Typora/typora-user-images/image-20241210165140103.png" alt="image-20241210165140103" style="zoom:67%;" /><br />正确率：最低，且比kmeans略低 |



## 四：EM算法训练GMM VS Kmeans

* 实现：K_means实现简单，主要需要调整的参数仅为簇的数量K。EM算法训练GMM实现较为复杂，参数包含均值、协方差和混合系数
* 效率：
  * K_means计算复杂度低，只需要计算样本间的距离，求和，取最大最小值等简单计算，所以训练速度快。
  * EM训练GMM聚类计算复杂度高：需要计算协方差矩阵，求特征值特征向量等复杂计算，训练速度低
* 收敛速度：K_means的收敛速度快，50次之内收敛。EM训练GMM的收敛速度慢，50次内无法收敛
* 正确率：在本次实验中，EM算法训练GMM模型在测试集达到的最终模型正确率更高，
  * GMM模型能够处理簇形状不规则、大小不一的数据集。
  * 每个高斯分布的均值和协方差矩阵决定了对应聚类的位置和形状，因此GMM模型能够灵活地适应不同形状的聚类。


​		



