## LLM

#### 1. 微调Fine-Tuning

Fine-Tuning的基本思想是采用已经在大量文本上进行训练的预训练语言模型，然后在小规模的任务特定上继续训练它。

#### 2. 提示微调Prompt-Tuning

* 特点：让下游任务去迁就预训练模型
* 步骤：构建模板，标签词预测，训练

##### 2.1 上下文学习 In-context learning

* 概述：ICL根据input生成提示和一些上下文演示（就是一些具体的例子），然后输入到语言模型中预测，不需要参数的梯度更新而是直接推理。这些提示和示例共同构建了一个上下文环境，引导模型根据这个环境来生成针对新任务的预测或响应。（这是在一个已经训练好的大模型上面进行微调）
  <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240727172619487.png" alt="image-20240727172619487" style="zoom:67%;" />
* **training：**在推理前，通过持续学习让语言模型的ICL能力得到进一步提升，这个过程称之为**model warmup**（模型预热），model warmup会优化语言模型的参数或者新增参数
* **Inference: **，同一个问题如果加上不同的示例，可能会得到不同的模型生成结果。

##### 2.2 Pattern-Verbalizer-Pair（PVP）

* pattern：带有mask标记的短文本
* Verbalizer：标签词的映射

##### 2.3 Prompt-Tuning

* Hard Prompt VS Soft Prompt：离散模板构建法的模板参数固定（每个任务都一样），连续模板构建法

#### 3. Instruction-Tuning（指示微调）

* Instruction的目的是告诉模型如何处理数据或执行某个操作，而不是简单地提供上下文或任务相关信息。


  因此，Prompt和instruction都是用于指导模型生成输出的文本，但它们的目的和使用方式是不同的。Prompt更多地用于帮助模型理解任务和上下文，而Instruction则更多地用于指导模型执行具体操作或完成任务。
#### 4. 思维链

思维链简单的说就是一系列中间推理步骤。这篇论文最大的贡献就是发现了在LLM生成推理任务的结果之前，先生成思维链，会使模型的推理性能有大幅度的提升，特别是在复杂的推理任务上，但是有个前提就是LLM的规模要大于10B，否则CoT没用甚至起副作用。

##### 4.1 人工思维链

<img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240811220902202.png" alt="image-20240811220902202" style="zoom:67%;" />

##### 4.2 Zero-shot-CoT（零示例思维链）

##### 4.3 Auto-CoT(自动思维链)

#### 5. PEFT 参数有效性微调

部分参数微调

##### 5.1

* Prefix-Tuning： 
* P-Tuning v2
* Adapter-Tuning: ，Adapter-Tuning 则是在预训练模型内部的网络层之间添加新的网络层或模块来适配下游任务。
* LoRA：在模型的Linear层的旁边，增加一个“旁支”，这个“旁支”的作用，就是代替原有的参数矩阵W WW进行训练。
*  对于左右两个部分，右侧看起来像是左侧原有矩阵W WW的分解，将参数量从$d × d $变成了$d × r + d × r $

* AdaLoRA：

#### 6 GPT

##### 6.1 gpt1

* 任务：自然语言推理，问答和常识推理，语义相似度，分类
* 模型结构：<img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240812132707676.png" alt="image-20240812132707676" style="zoom:67%;" />
* 模型训练：1）在大规模无标注文本数据上学习到一个高容量的语言模型；2）在标注数据上进行微调。其中第二步是针对具体的下游任务来进行训练的。
  * 无监督训练
  * 监督训练: 
  * 下游任务：
    * 分类任务：将起始和终止token加入到原始序列两端，输入transformer中得到特征向量，最后经过一个全连接得到预测的概率分布；
    * 自然语言推理：将前提（premise）和假设（hypothesis）通过分隔符（Delimiter）隔开，两端加上起始和终止token。再依次通过transformer和全连接得到预测结果；
    * 语义相似度：输入的两个句子，正向和反向各拼接一次（由于相似性质是对称的，为了消除顺序的影响），然后分别输入给transformer，得到的特征向量拼接后再送给全连接得到预测结果；
    * 问答和常识推理：将个选项的问题抽象化为个二分类问题，即每个选项分别和内容进行拼接，然后各送入transformer和全连接中，最后选择置信度最高的作为预测结果。

##### 6.2 gpt2

* 模型训练：

* <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240812134459804.png" alt="image-20240812134459804" style="zoom:67%;" />

  模型训练：只有预训练过程

  * 无监督训练：在下游任务中，不采用Fine-Tuning方法，而是采用zero-shot方法
  * 下游任务：为什么可以这么讲呢？因为作者认为：下游任务 (有监督训练) 可以视为预训练过程 (无监督训练) 的一个子集。无监督目标的全局最优解也是有监督训练的全局最优解。当预训练规模足够大时，把无监督的任务训练好了，有监督的下游任务即不再需要额外训练，就是所谓的zero-shot。

##### 6.3 gpt3

* **模型训练**：GPT-3也只有预训练过程，GPT-3采用了In-context learning。

* 下游任务：

  

## LERAGING REINFORCEMENT LEARNING AND LARGE LANGUAGE MODELS FOR CODE OPTIMIZATION

*  问题：利用大模型和强化学习辅助代码优化，但是训练数据集都是通用的，无法满足需求，或者没有利用好强化学习和环境的反馈交互

*  挑战：
  1. 如何将测试的反馈用于LLM的训练
  2. 如何使小模型的性能和大模型的性能相似
  3. 如何使小模型能生成少错误的可靠代码和解决完成代码优化任务

*  框架：

   *  大模型微调：

      *  codeT5（自然语言混合编程语言的输入）
      *  微调目标：最小化交叉熵损失![image-20240723163401014](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240723163401014.png)

   *  样本生成：（采什么的样？？）

      *  策略：贪心采样，随机采样

         贪心策略：topB约束

   * 强化学习：评分模型，奖励模型![image-20240723173446617](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240723173446617.png)

     * reward：![image-20240723174255589](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240723174255589.png)

     * score：

       ![image-20240723174408255](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240723174408255.png)



## RLADAPTER: BRIDGING LARGE LANGUAGE MODELS TO REINFORCEMENT LEARNING IN OPEN WORLDS  

* key: 考虑增加可调节模块来帮助llm适应环境。 这种见解促使提出RLAdapter框架， 旨在增强RL算法
  和llm之间的协作。 (不用修改LLM)![image-20240723215431938](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240723215431938.png)

#### 3 方法

##### 框架

##### <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240812190518471.png" alt="image-20240812190518471" style="zoom:67%;" />

* 关注于添加可调节模块来帮助llm灵活地适应环境，而不是直接修改llm。

* 适配性模型：

  * 输入：关于环境的基本信息和agent当前对语言指导的理解水平u
  * 将输入整合成适配器模型的提示，提取重要细节，生成汇总信息

* 训练流程：

  * LLM接收到适应的提示c ~ Mada(提示(B, u))时，生成g ~ MLLM(提示(B, c))，随后将其提供给策略π(a|o, gemb)进行训练，其中gemb是由femb编码的文本嵌入

  * 在查询语言模型之间设置预定的间隔，在间隔期间保持一致的指导
    ![image-20240812201407706](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240812201407706.png)
  * 初始化阶段：
    * **策略 π**：初始化一个策略，用于在给定当前状态和环境生成文本（`gt`）的条件下选择动作。
    * **缓冲区 B**：用于存储交互历史，包括观察（`ot`）、动作（`at`）、下一个观察（`ot+1`）、奖励（`rt`）和生成的文本（`gt`）。
    * **SFT 缓冲区 D**：用于存储需要用于监督微调（SFT）的样本。
    * **参数**：设置 LLM 生成文本的间隔 `Ngen`、SFT 的间隔 `Nsft` 以及 ROGUE-L 阈值 `θ`，用于评估生成的文本质量。
  * 生成阶段
    * **步骤 5-9**：每隔 `Ngen` 时间步，使用 `Mada`（可能是指某种适配器模型）和 LLM（大型语言模型）来生成文本 `ct` 和 `gt`。如果未到达生成间隔，则复用前一次的生成结果
  * 环境交互阶段
    * **步骤 10-11**：根据当前策略和生成的文本 `gt` 选择动作 `at`，并与环境交互获得下一个观察 `ot+1`。

  * 更新阶段

    * **步骤 12**：更新缓冲区 B，加入新的交互数据。

    * **步骤 13**：使用缓冲区 B 的数据更新策略 π。

  * 理解和微调阶段

    * **步骤 16**：计算当前生成文本 `gt` 与缓冲区 B 中随机选取的样本 `τ` 的嵌入向量之间的余弦相似度 `ut+1`，作为对生成文本理解程度的评估。

    * **步骤 17-19**：如果 `gt` 的质量（通过 ROGUE-L 分数衡量）低于阈值 `θ`，则将其与相应的提示一起加入 SFT 缓冲区 D。

    * **步骤 20-23**：每隔 `Nsft` 时间步，使用 SFT 缓冲区 D 中的数据对适配器模型 `Mada` 进行监督微调，以提高其生成文本的质量和适应性。

#### 4 实验

* 目的：

  • 适配器模型的集成可以增强大型语言模型对下游任务的理解和智能体的理解能力， 从
  而产生更有意义的指导。
  • 在RLAdapter框架下训练的智能体可以表现出更出色的性能， 并表现出更符合常识的行
  为。  

  

## ML-based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection（注入故障）

故障注入（Fault Injection）是一种重要的测试技术，主要用于评估系统（包括软件和硬件）在面临异常情况时的可靠性和稳定性。通过人为地向系统中引入故障，并观察系统在这些故障条件下的行为，可以检测出潜在的设计缺陷、恢复机制失效以及安全漏洞等问题，从而改进系统设计，增强其抵抗真实故障的能力



介绍了故障注入工具DriveFI， 以及经验评估ADS故障传播、 弹性和安全特性的方法， 以及生成和测试角落案例故障条件的方法。 DriveFI结合了贝叶斯和传统的FI框架， 它们协同工作以加速发现安全关键故障。  

#### 1 方法概述

* 自动驾驶汽车基本架构：

  由机械部件和执行器组成， 由ADS控制， 它代表自动驾驶的计算(硬件和软件)组件。 在时间t的每一个瞬间， ADS系统从传感器It(例如， 摄像头， 激光雷达， GPS)输入， 从机械部件(例如， 速度vt， 加速度at))获取惯性测量Mt， 并推断驱动命令At(例如， 油门ζ， 制动b， 转向角φ

  将ADS细分为两个组件:(a) ML模块(负责感知和规划)， 将It和Mt作为输入并产生原始驱动命令UA,t， 以(b) PID控制器[33]， 负责平滑输出UA,t以产生  

    <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240813110629166.png" alt="image-20240813110629166" style="zoom: 80%;" />

* 安全：
  * 停车距离dstop被定义为在应用最大舒适减速amax时，车辆在完全停车前行驶的最大距离。
  * AV车辆的安全包络dsafe[1]， [2]定义为AV车辆在不与任何静态或动态物体发生碰撞的情况下所能行驶的最大距离
  * 安全潜力δ定义为δ=dsafe-dstop。 当横向和纵向δ均> 0时， AV处于安全状态。

* 故障注入：
  * 输出：直接将错误注入ADS输出
  * 将错误注入到内存单元中，但变量被损坏以模拟故障

* 案例（用贝叶斯故障注入的重要性而不是随机注入）：
  * 危险错误：要用贝叶斯故障注入精确注入故障
  * Real-World Crash：特斯拉自驾仪引起的现实故障世界

  #### 2. 贝叶斯故障注入

* 基于运动学的安全模型：

  <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240813175316829.png" alt="image-20240813175316829" style="zoom:80%;" />

  （车辆在时刻t有一个瞬时位置(xt;Yt)，速度vt，航向θt，转向角φt。）

  $dx_t/dt = v_t cos θ_t; dy_t/dt = v_t sin θ_t; dθ_t/dt = (v_t tan φ_t)=L(L是两个轮子间的距离)  $​

  （只考虑二维简单的情况）

  * 紧急停车机动：d_stop 龙格库达法计算，d_safe传感器测

  * 离散化：离散时间是ADS的自然选择， 因为控制决策是在与传感器采样频率相对应的离散步骤上做出的。 

* ML model (机器学习模型)
  * 目标：目标故障注入器的目标是找到δ> 0， 但在向ADS堆栈注入故障f(表现为EV运动状态的变化)的情况下， δdo(f)≤0。  （找到临界值）

  * 模型：估计时间k + 1处dstop的值 ，需要知道xk+1、 yk+1、 vk+1、 θk+1和φk+1的值作为启动紧
    急停止机动的初始条件  

    * DriveFI使用动态贝叶斯网络(DBN)[41]， 特别是3-Temporal贝叶斯网络(TBN)， 即展开三次的DBN， 对xk+1、yk+1、 vk+1、 θk+1和φk+1进行建模。 ![image-20240825144233925](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240825144233925.png) 

    * 概率推理：先在无障碍条件下执行模拟，得到其他变量的正确运行值。首先通过使用马尔可夫链蒙特卡洛方法[41]估计vk+1的后验分布， 然后估计vk+1的最有可能值。其他值类似相同的方法

      最后得到：<img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240825154114912.png" alt="image-20240825154114912" style="zoom:67%;" />（这个估计是拿来找到不安全状态，然后找到临界故障的）

  * 训练：找到最契合训练数据的$\theta $

    ![image-20240825151559973](C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240825151559973.png)

    * **拓展**：对数概率的期望能够帮助检测模型的拟合性（对数概率 log*P*(*X*∣*μ*,*σ*) 代表了在给定模型参数 *μ* 和 *σ* 下观测到数据 *X* 的“惊喜度”的负对数。当概率 *P*(*X*∣*μ*,*σ*) 较高时，对数概率也较高（尽管是负的），表示观测到 *X* 并不那么“意外”。相反，当概率较低时，对数概率也较低，表示观测到 *X* 是比较“意外”的。）
      对数概率的期望实际上是对数似然函数（log-likelihood function）的期望值，它衡量了模型在给定的参数下，生成观测数据的“合理性”或“可能性”。当对数概率的期望较高时，意味着模型能够很好地解释或拟合观测到的数据。这通常表明模型的参数设置得当，能够捕捉到数据中的关键特征。

  * 训练数据获取：

    * Xk中的变量是通过在模拟器中多个驾驶场景中执行ADS来测量的  
    * 驾驶场景中注入随机障碍

  * 障碍注射

    * 先准备好一个没有障碍的模拟执行，得到黄金参数

    * 用黄金参数搭建好模拟场景

    * 在模拟场景中注入故障（离线执行每个模拟时间点的BN MLE推断， 以找到关键故障集  ）

      <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240825153328922.png" alt="image-20240825153328922" style="zoom:67%;" />

* ADS架构和仿真（autonomous driving system  自动驾驶系统）

  *  AI平台：
    * 传感器抽象层：输入数据的预处理、噪声滤波、增益控制[45]、音调映射[46]、去马赛克[47]，以及根据传感器类型提取感兴趣的区域。
    * 感知层：处理传感器抽象层传入数据，使用计算机视觉技术(包括深度学习[48])来检测静态物体(如车道、交通标志、障碍物)和动态物体
      * 任务：分割，分类，聚类，对象和车道的实时跟踪，计算各种有用的指标（定义世界模型）
    * Localization Layer  定位层：收集各种数据，在世界模型里定位自动驾驶车辆
    * Prediction Layer   预测层：使用来自世界模型的信息(例如，位置，标题，速度，加速度)为检测到的物体生成轨迹，可以概率地识别自动驾驶汽车路径上的障碍物。
    * Planning & Control Layer    计划控制层：生成导航计划，并向自动驾驶汽车发送控制信号(驱动、制动、转向)
      * 路由模块” 根据请求生成高级导航信息。
        路由模块需要知道路由起点和路由终点， 以便计算通道车
        道和道路。 “规划模块” 通过使用定位输出、 预测输出和
        路由输出， 规划出安全无碰撞的轨迹。 “控制模块” 以规
        划的轨迹为输入， 生成控制命令传递给CAN总线， CAN总
        线将信息传递给AV的机械部件。  
  * 仿真平台：


* V. DRIVEFI架构  

  <img src="C:\Users\丁晓琪\AppData\Roaming\Typora\typora-user-images\image-20240825162926731.png" alt="image-20240825162926731" style="zoom:80%;" />

  * 场景协调管理器，活动管理器，事件驱动同步模块
  * 注入计算元素:GPU故障模型        
    * 故障注入：指令输出时注入比特翻转
  * 将故障注入ADS模块输出变量：
    * SLI源极注射：修改ADS模块的输出变量，破坏ADS内部状态，需要源码修改和重新编译
    * 1-固定：在给定ADS软件模块用一个恒定值来破坏给定的ADS软件模块输出，第k个场景注入单个故障。   
    * m-固定：从场景k开始， 将m个故障注入给定的一组ADS软件模块输出， 并继续将故障注入ADS软件模块输出， 直到场景k + m。  
    * 1-Random  ：在均匀随机选择的一组ADS模块输出中， 在第k个场景注入单个故障。 注入的故障值也是均匀随机选择的  
    * 从场景k开始， 在一组随机选择的ADS软件模块输出中注入m个故障， 并继续在ADS软件模块输出中注入故障， 直到场景k + m。   

* 结果：

  

## Llumnix: Dynamic Scheduling for Large Language Model Serving  

#### 摘要

* 问题：大语言模型的系统有严重的排队延迟、 糟糕
  的尾部延迟和违反SLO等问题

* Llumnix:  重新调度请求以改善负载平衡和隔离， 减
  轻资源碎片， 并区分请求优先级和slo  

#### 介绍

* 工作负载异构性（workload heterogeneity  ）：问题多样性导致LLM推理工作的异构性
* 执行的不可预测性（execution unpredictability）：由于迭代，请求的执行时间和资源需求是不可预测的
* 更类似于现代操作系统， 它托管具有动态工作集和多核上不同优先级的进程。 管理这样的系统有复杂的目标  
* 难以实现性能隔离：
  Performance Isolation通过物理或逻辑手段，将系统资源（如CPU、内存、网络带宽等）分配给不同的任务或进程，确保它们各自拥有足够的资源来执行任务，而不会相互干扰。其主要目的是提高系统的整体性能和稳定性，同时确保关键任务或高优先级任务能够获得足够的资源保障。
* 优先级：处理时通常请求的优先级一致，但是实际请求的延迟要求不一样
* lumnix采用分布式调度架构

#### 背景

* LLM的应用多样性

* LLM自回归生成（Autoregressive generation  ）：

  * 序列的每一个元素（如文本中的单词、时间序列中的时间点等）都是基于序列中之前所有元素的信息来预测的；每一步都依赖于之前已经生成的序列部分

  * 模型迭代地接受输入序列加上之前的所有输出标记以生成下一个输出标记
  * prefill（预填充）延迟决定和了开始接受响应需要的时间；decode(解码)延迟决定接下来接受速度

* 批处理和内存管理

  * 连续批处理：Continuous Batching允许将一个或多个推理请求组合成单个批次，以最大化吞吐量。与传统的静态批处理不同，静态批处理在推理完成之前批大小保持不变，而Continuous Batching则允许批大小在推理过程中动态变化新的/完成的请求可以立即加入/离开正在运行的批处理，而不是等待所有正在运行的请求完成
  * KV缓存：先验未知-->动态内存分配
    它存储了模型中每个token（词元）的键（Key）和值（Value）向量，这些向量在模型进行连续token生成时，能够避免重复计算，从而提高推理效率。

#### 动机








​                                       

