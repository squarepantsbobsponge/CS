# 问题

数据结构，树的数据结构--每个树的节点给个提示，分支分类是什么，下边的分支节点也表明自己的类别

怎么高效率判断样本集中的样本是否全部属于同一个类别和是否属性都相同--样本计算时的数据结构，label和特征不分开，A就是0，1，2，3

计算下一个分支，信息增益率---怎么高效计算，给个字典汇总计算

## 代码实现与检查

树的结构

```python
class Node:
    def _init_(self,value,data):#value是父节点分支属性下自己的类别是什么
        self.value=value
        self.children=[]#子节点
        self.sort=None #下一个要分支的属性
        #要不要往里面放data呢
        self.data=data
        self.ent=0 #当层信息熵
        self.leave=-1 #0为0类，1为1类，-1为分支
```

数据处理：

```python
    def __init__(self,filename:string) :#读取数据，整理数据，设置参数初始值和参数数量
        #提取信息
        f=open(filename)
        csv_reader=csv.reader(f)
        self.data=[]#存储提取出来的数据，格式为【【A，E，P】, ....】
        flag=1#去掉头信息
        attribute_1=[]##属性集 [[]]
        attribute_2=[]##
        attribute_3=[]
        attribute_4=[]
        for line in csv_reader:
            if flag==1:
                flag=0
                continue
            #提取整理数据 #不将数据数字化了，直接上，避免测试集还要处理
            for i in range(0,len(line)):
                if(i==0 and line[i] not in attribute_1):
                    attribute_1.append(line[i])
                if(i==1 and line[i] not in attribute_2):
                    attribute_2.append(line[i])
                if(i==2):#对标签处理以下
                    line[i]=int(line[i])
                    if(line[i]<=3000):
                        line[i]=0
                    else:
                        line[i]=1 #标签变成0，1
                if(i==3):#字符串转数字
                    line[i]=int(line[i])//10   
                    if(line[i] not in attribute_3): 
                        attribute_3.append(line[i]) ##这里要离散化处理  每10年一个单位
                if(i==4 and line[i] not in attribute_4):
                    attribute_4.append(line[i])   
            self.data.append(line)   
        self.attribute=[attribute_1,attribute_2,[],attribute_3,attribute_4]   #空一位后续便于直接按照[]索引对照data[]索引                 
        self.head=self.Tree_Generate(self.data,self.attribute,0,[])
```

总体构建树的逻辑：

```python
    def Tree_Generat (self,data,attribute,value,no_attribute):
        node=Node(value,data)
        #当层信息熵计算
        node.ent,sum_1,sum_0=self.Ent(data)
        #终止条件1:如果data中的数据全部属于同一类别，那直接标记为C类叶子节点
        #终止条件2:data在所有属性上取值相同
        flag1=data[0][2] #兼职代表
        flag2=data[0]
        flag=1
        for i in range(1,len(data)):
            if data[i][2]!=flag1:
                flag1=data[i][2]
            for j in range(0,5):#判断属性是否从一而终
                if(flag2[j]!=data[i][j] and j!=2):
                    flag=0
            if(flag1!=data[0][2] and flag==0):
                    break
        if(flag1==data[0][2]):##从头到尾的标签都一样，return为叶子节点
            node.leave=flag1
            return node
        if(flag==1):##是否所有属性全部相同
            node.leave=1 if sum_1>sum_0 else 0
            return node
        ##挑选最佳分类属性
        a_best=self.compute_IGR(attribute,node,no_attribute) #返回分类属性的索引
        ##按照最佳属性分支
        #先把data分好
        a_sort={} #键为取值，value为样本
        #初始化
        for atrri in attribute[a_best]:
            a_sort[atrri]=[]
        #分数据
        for data_index in data:
            a_sort[data_index[a_best]].append(data_index)
        #分支
        for atrri in attribute[a_best]:
            d_v=a_sort[atrri]
            if(len(d_v)==0):
                child_node=Node(0,data)
                child_node.leave=1 if sum_1>sum_0 else 0
                node.children.append(child_node)
            else:
                ##要是attribute的子集，但是前面都直接拿它当索引了
                no_attribute.append(a_best)
                child_node=self.Tree_Generat(d_v,attribute,atrri,no_attribute)
                node.children.append(child_node)
                no_attribute.remove(a_best) #这是递归式的
        return node 
```

信息熵的计算：

```python
    def Ent(self,data):#信息熵的计算
        sum_1=0
        sum_0=0 #统计标签数
        for i in range(0,len(data)):
            if(data[i][2]==0):
                sum_0+=1
            else:
                sum_1+=1
        ##统计概率
        p_1=sum_1/(sum_1+sum_0)
        p_0=sum_0/(sum_0+sum_1)
        ##计算熵
        ent=-p_1*math.log2(p_1)-p_0*math.log2(p_0)
        return ent,sum_1,sum_0
```

信息增益率计算和挑选出最好的分类属性：

```python
    def compute_IGR(self,attribute,node:Node,no_attribute):#计算属性的信息增益率
        #先建立个属性取值字典，key是该属性的具体取值，value是[0,0，0]该取值的总数，和该取值下，0和1的个体数
        ##感觉可以一起求，少遍历
        #初始化完成
        data=node.data
        cur_ent=node.ent
        best_IGR=-1000 #最好的信息增益率
        best_attri=-1
        dict=[{},{},{},{},{}]#空一个便于索引
        for i in range(0,len(attribute)):
            if i==2 :continue
            for j in attribute[i]:
                dict[i][j]=[0,0,0]
        ##遍历数据，整理数据
        for data_singel in data:
            for i in range(0,5):
                if i==2:continue
                dict[i][data_singel[i]][0]+=1
                if data_singel[2]==0:#分类标签为0
                    dict[i][data_singel[i]][1]+=1
                else:
                    dict[i][data_singel[i]][2]+=1
        ##算每个属性的信息增益率
        for i in range(0,len(dict)):
            if i==2 or i in no_attribute:continue
            dict_index=dict[i]
            count_ent=0
            IV=0
            #先算信息增益
            sum=len(data) #数据总数
            for key,item in dict_index: #属性下的单类别
                p_0=item[1]/item[0]
                p_1=item[2]/item[0]
                p=item[0]/sum
                d_v_ent=-p_0*math.log2(p_0)-p_1*math.log2(p_1)
                count_ent+=(p*d_v_ent)
                d_v_IV=p*math.log2(p)
                IV-=d_v_IV
            Gain=cur_ent-count_ent
            IGR=Gain/IV
            if(IGR>=best_IGR):
                best_IGR=IGR
                best_attri=i
        node.sort=best_attri
        return best_attri
```

存在问题：没剪枝，过拟合

存在问题：分到的类里可能一个样本也没有，除0 错误

存在问题：类里的标签可能全部为1或者全部为0这是log对数错误，信息熵为0  解决

存在问题：拉普拉斯平滑